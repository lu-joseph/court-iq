{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/TrackNetV3\n",
      "/root\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"TrackNetV3\"):\n",
    "    !git clone https://github.com/lu-joseph/TrackNetV3\n",
    "%cd {HOME}/TrackNetV3\n",
    "if not os.path.exists(\"ckpts\"):\n",
    "    !wget --no-check-certificate 'https://drive.usercontent.google.com/download?id=1CfzE87a0f6LhBp0kniSl1-89zaLCZ8cA&export=download&authuser=0&confirm=t&uuid=fd5c99d1-dc6a-4011-948b-1d03be7d6628&at=AN_67v1rreyiyg-hMrVlkh9Np5is%3A1729497024731' -O TrackNetV3_ckpts.zip\n",
    "    !unzip TrackNetV3_ckpts.zip\n",
    "%cd {HOME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/TrackNetV3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dash==2.5.1 (from -r requirements.txt (line 1))\n",
      "  Using cached dash-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy==1.24.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.24.1)\n",
      "Collecting opencv_python==4.4.0.46 (from -r requirements.txt (line 3))\n",
      "  Using cached opencv_python-4.4.0.46-cp38-cp38-manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting pandas==2.0.0 (from -r requirements.txt (line 4))\n",
      "  Using cached pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting Pillow==10.0.0 (from -r requirements.txt (line 5))\n",
      "  Using cached Pillow-10.0.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting plotly==5.8.2 (from -r requirements.txt (line 6))\n",
      "  Using cached plotly-5.8.2-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (2.0.0+cu118)\n",
      "Collecting parse (from -r requirements.txt (line 8))\n",
      "  Using cached parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting Flask>=1.0.4 (from dash==2.5.1->-r requirements.txt (line 1))\n",
      "  Using cached flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting flask-compress (from dash==2.5.1->-r requirements.txt (line 1))\n",
      "  Using cached Flask_Compress-1.15-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting dash-html-components==2.0.0 (from dash==2.5.1->-r requirements.txt (line 1))\n",
      "  Using cached dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting dash-core-components==2.0.0 (from dash==2.5.1->-r requirements.txt (line 1))\n",
      "  Using cached dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting dash-table==5.0.0 (from dash==2.5.1->-r requirements.txt (line 1))\n",
      "  Using cached dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas==2.0.0->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==2.0.0->-r requirements.txt (line 4)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas==2.0.0->-r requirements.txt (line 4)) (2024.2)\n",
      "Collecting tenacity>=6.2.0 (from plotly==5.8.2->-r requirements.txt (line 6))\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch==2.0.0->-r requirements.txt (line 7)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==2.0.0->-r requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch==2.0.0->-r requirements.txt (line 7)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch==2.0.0->-r requirements.txt (line 7)) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch==2.0.0->-r requirements.txt (line 7)) (3.1.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch==2.0.0->-r requirements.txt (line 7)) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 7)) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 7)) (15.0.7)\n",
      "Collecting Werkzeug>=3.0.0 (from Flask>=1.0.4->dash==2.5.1->-r requirements.txt (line 1))\n",
      "  Using cached werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask>=1.0.4->dash==2.5.1->-r requirements.txt (line 1))\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.8/dist-packages (from Flask>=1.0.4->dash==2.5.1->-r requirements.txt (line 1)) (8.1.7)\n",
      "Collecting blinker>=1.6.2 (from Flask>=1.0.4->dash==2.5.1->-r requirements.txt (line 1))\n",
      "  Using cached blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from Flask>=1.0.4->dash==2.5.1->-r requirements.txt (line 1)) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch==2.0.0->-r requirements.txt (line 7)) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting brotli (from flask-compress->dash==2.5.1->-r requirements.txt (line 1))\n",
      "  Using cached Brotli-1.1.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting zstandard (from flask-compress->dash==2.5.1->-r requirements.txt (line 1))\n",
      "  Using cached zstandard-0.23.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch==2.0.0->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=3.6.0->Flask>=1.0.4->dash==2.5.1->-r requirements.txt (line 1)) (3.20.2)\n",
      "Using cached dash-2.5.1-py3-none-any.whl (9.8 MB)\n",
      "Using cached opencv_python-4.4.0.46-cp38-cp38-manylinux2014_x86_64.whl (49.5 MB)\n",
      "Using cached pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Using cached Pillow-10.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Using cached plotly-5.8.2-py2.py3-none-any.whl (15.2 MB)\n",
      "Using cached dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
      "Using cached dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
      "Using cached dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
      "Using cached parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
      "Using cached flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached Flask_Compress-1.15-py3-none-any.whl (8.6 kB)\n",
      "Using cached blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "Using cached Brotli-1.1.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\n",
      "Using cached zstandard-0.23.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "Installing collected packages: parse, dash-table, dash-html-components, dash-core-components, brotli, zstandard, Werkzeug, tenacity, Pillow, opencv_python, itsdangerous, blinker, plotly, pandas, Flask, flask-compress, dash\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 10.2.0\n",
      "    Uninstalling pillow-10.2.0:\n",
      "      Successfully uninstalled pillow-10.2.0\n",
      "  Attempting uninstall: opencv_python\n",
      "    Found existing installation: opencv-python 4.10.0.84\n",
      "    Uninstalling opencv-python-4.10.0.84:\n",
      "      Successfully uninstalled opencv-python-4.10.0.84\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "inference 0.24.0 requires networkx>=3.1, but you have networkx 3.0 which is incompatible.\n",
      "inference 0.24.0 requires opencv-python<=4.10.0.84,>=4.8.1.78, but you have opencv-python 4.4.0.46 which is incompatible.\n",
      "inference-gpu 0.24.0 requires networkx>=3.1, but you have networkx 3.0 which is incompatible.\n",
      "inference-gpu 0.24.0 requires opencv-python<=4.10.0.84,>=4.8.1.78, but you have opencv-python 4.4.0.46 which is incompatible.\n",
      "ultralytics 8.3.21 requires opencv-python>=4.6.0, but you have opencv-python 4.4.0.46 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Flask-3.0.3 Pillow-10.0.0 Werkzeug-3.0.4 blinker-1.8.2 brotli-1.1.0 dash-2.5.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 flask-compress-1.15 itsdangerous-2.2.0 opencv_python-4.4.0.46 pandas-2.0.0 parse-1.20.2 plotly-5.8.2 tenacity-9.0.0 zstandard-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m/root\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}/TrackNetV3\n",
    "!pip install -r requirements.txt\n",
    "%cd {HOME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "HEIGHT = 288\n",
    "WIDTH = 512\n",
    "SIGMA = 2.5\n",
    "DELTA_T = 1/math.sqrt(HEIGHT**2 + WIDTH**2)\n",
    "COOR_TH = DELTA_T * 50\n",
    "IMG_FORMAT = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/TrackNetV3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34/2260988115.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tracknet_ckpt = torch.load(tracknet_file)\n",
      "/tmp/ipykernel_34/2260988115.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inpaintnet_ckpt = torch.load(inpaintnet_file)\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}/TrackNetV3\n",
    "from predict import predict\n",
    "from utils.general import get_model\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "video_file = '../rally.mp4'\n",
    "video_name = video_file.split('/')[-1][:-4]\n",
    "tracknet_file = 'ckpts/TrackNet_best.pt'\n",
    "inpaintnet_file = 'ckpts/InpaintNet_best.pt' \n",
    "save_dir = \"prediction\"\n",
    "if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "out_csv_file = os.path.join(save_dir, f'{video_name}_ball.csv')\n",
    "batch_size = 12\n",
    "num_workers = batch_size if batch_size <= 16 else 16\n",
    "eval_mode = \"weight\"\n",
    "\n",
    "\n",
    "tracknet_ckpt = torch.load(tracknet_file)\n",
    "tracknet_seq_len = tracknet_ckpt['param_dict']['seq_len']\n",
    "bg_mode = tracknet_ckpt['param_dict']['bg_mode']\n",
    "tracknet = get_model('TrackNet', tracknet_seq_len, bg_mode).cuda()\n",
    "tracknet.load_state_dict(tracknet_ckpt['model'])\n",
    "\n",
    "inpaintnet_ckpt = torch.load(inpaintnet_file)\n",
    "inpaintnet_seq_len = inpaintnet_ckpt['param_dict']['seq_len']\n",
    "inpaintnet = get_model('InpaintNet').cuda()\n",
    "inpaintnet.load_state_dict(inpaintnet_ckpt['model'])\n",
    "\n",
    "cap = cv2.VideoCapture(video_file)\n",
    "w, h = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "w_scaler, h_scaler = w / WIDTH, h / HEIGHT\n",
    "img_scaler = (w_scaler, h_scaler)\n",
    "\n",
    "tracknet_pred_dict = {'Frame':[], 'X':[], 'Y':[], 'Visibility':[], 'Inpaint_Mask':[],\n",
    "                    'Img_scaler': (w_scaler, h_scaler), 'Img_shape': (w, h)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/TrackNetV3\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}/TrackNetV3\n",
    "from utils.general import generate_frames\n",
    "from dataset import Shuttlecock_Trajectory_Dataset, Video_IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "tracknet.eval()\n",
    "seq_len = tracknet_seq_len\n",
    "\n",
    "# Sample all frames from video\n",
    "frame_list = generate_frames(video_file)\n",
    "dataset = Shuttlecock_Trajectory_Dataset(seq_len=seq_len, sliding_step=1, data_mode='heatmap', bg_mode=bg_mode,\n",
    "                                        frame_arr=np.array(frame_list)[:, :, :, ::-1])\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "video_len = len(frame_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/TrackNetV3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 21/21 [00:28<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}/TrackNetV3\n",
    "from utils.test import get_ensemble_weight\n",
    "from tqdm import tqdm\n",
    "# Init prediction buffer params\n",
    "num_sample, sample_count = video_len-seq_len+1, 0\n",
    "buffer_size = seq_len - 1\n",
    "batch_i = torch.arange(seq_len) # [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "frame_i = torch.arange(seq_len-1, -1, -1) # [7, 6, 5, 4, 3, 2, 1, 0]\n",
    "y_pred_buffer = torch.zeros((buffer_size, seq_len, HEIGHT, WIDTH), dtype=torch.float32)\n",
    "weight = get_ensemble_weight(seq_len, eval_mode)\n",
    "for step, (i, x) in enumerate(tqdm(data_loader)):\n",
    "    x = x.float().cuda()\n",
    "    b_size, seq_len = i.shape[0], i.shape[1]\n",
    "    with torch.no_grad():\n",
    "        y_pred = tracknet(x).detach().cpu()\n",
    "    \n",
    "    y_pred_buffer = torch.cat((y_pred_buffer, y_pred), dim=0)\n",
    "    ensemble_i = torch.empty((0, 1, 2), dtype=torch.float32)\n",
    "    ensemble_y_pred = torch.empty((0, 1, HEIGHT, WIDTH), dtype=torch.float32)\n",
    "\n",
    "    for b in range(b_size):\n",
    "        if sample_count < buffer_size:\n",
    "            # Imcomplete buffer\n",
    "            y_pred = y_pred_buffer[batch_i+b, frame_i].sum(0) / (sample_count+1)\n",
    "        else:\n",
    "            # General case\n",
    "            y_pred = (y_pred_buffer[batch_i+b, frame_i] * weight[:, None, None]).sum(0)\n",
    "        \n",
    "        ensemble_i = torch.cat((ensemble_i, i[b][0].reshape(1, 1, 2)), dim=0)\n",
    "        ensemble_y_pred = torch.cat((ensemble_y_pred, y_pred.reshape(1, 1, HEIGHT, WIDTH)), dim=0)\n",
    "        sample_count += 1\n",
    "\n",
    "        if sample_count == num_sample:\n",
    "            # Last batch\n",
    "            y_zero_pad = torch.zeros((buffer_size, seq_len, HEIGHT, WIDTH), dtype=torch.float32)\n",
    "            y_pred_buffer = torch.cat((y_pred_buffer, y_zero_pad), dim=0)\n",
    "\n",
    "            for f in range(1, seq_len):\n",
    "                # Last input sequence\n",
    "                y_pred = y_pred_buffer[batch_i+b+f, frame_i].sum(0) / (seq_len-f)\n",
    "                ensemble_i = torch.cat((ensemble_i, i[-1][f].reshape(1, 1, 2)), dim=0)\n",
    "                ensemble_y_pred = torch.cat((ensemble_y_pred, y_pred.reshape(1, 1, HEIGHT, WIDTH)), dim=0)\n",
    "\n",
    "    # Predict\n",
    "    tmp_pred = predict(ensemble_i, y_pred=ensemble_y_pred, img_scaler=img_scaler)\n",
    "    for key in tmp_pred.keys():\n",
    "        tracknet_pred_dict[key].extend(tmp_pred[key])\n",
    "\n",
    "    # Update buffer, keep last predictions for ensemble in next iteration\n",
    "    y_pred_buffer = y_pred_buffer[-buffer_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/TrackNetV3\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}/TrackNetV3\n",
    "from utils.test import generate_inpaint_mask\n",
    "\n",
    "inpaintnet.eval()\n",
    "seq_len = inpaintnet_seq_len\n",
    "tracknet_pred_dict['Inpaint_Mask'] = generate_inpaint_mask(tracknet_pred_dict, th_h=h*0.05)\n",
    "inpaint_pred_dict = {'Frame':[], 'X':[], 'Y':[], 'Visibility':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 21/21 [00:01<00:00, 16.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create dataset with overlap sampling for temporal ensemble\n",
    "dataset = Shuttlecock_Trajectory_Dataset(seq_len=seq_len, sliding_step=1, data_mode='coordinate', pred_dict=tracknet_pred_dict)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
    "weight = get_ensemble_weight(seq_len, eval_mode)\n",
    "\n",
    "# Init buffer params\n",
    "num_sample, sample_count = len(dataset), 0\n",
    "buffer_size = seq_len - 1\n",
    "batch_i = torch.arange(seq_len) # [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "frame_i = torch.arange(seq_len-1, -1, -1) # [7, 6, 5, 4, 3, 2, 1, 0]\n",
    "coor_inpaint_buffer = torch.zeros((buffer_size, seq_len, 2), dtype=torch.float32)\n",
    "\n",
    "for step, (i, coor_pred, inpaint_mask) in enumerate(tqdm(data_loader)):\n",
    "    coor_pred, inpaint_mask = coor_pred.float(), inpaint_mask.float()\n",
    "    b_size = i.shape[0]\n",
    "    with torch.no_grad():\n",
    "        coor_inpaint = inpaintnet(coor_pred.cuda(), inpaint_mask.cuda()).detach().cpu()\n",
    "        coor_inpaint = coor_inpaint * inpaint_mask + coor_pred * (1-inpaint_mask)\n",
    "    \n",
    "    # Thresholding\n",
    "    th_mask = ((coor_inpaint[:, :, 0] < COOR_TH) & (coor_inpaint[:, :, 1] < COOR_TH))\n",
    "    coor_inpaint[th_mask] = 0.\n",
    "\n",
    "    coor_inpaint_buffer = torch.cat((coor_inpaint_buffer, coor_inpaint), dim=0)\n",
    "    ensemble_i = torch.empty((0, 1, 2), dtype=torch.float32)\n",
    "    ensemble_coor_inpaint = torch.empty((0, 1, 2), dtype=torch.float32)\n",
    "    \n",
    "    for b in range(b_size):\n",
    "        if sample_count < buffer_size:\n",
    "            # Imcomplete buffer\n",
    "            coor_inpaint = coor_inpaint_buffer[batch_i+b, frame_i].sum(0)\n",
    "            coor_inpaint /= (sample_count+1)\n",
    "        else:\n",
    "            # General case\n",
    "            coor_inpaint = (coor_inpaint_buffer[batch_i+b, frame_i] * weight[:, None]).sum(0)\n",
    "        \n",
    "        ensemble_i = torch.cat((ensemble_i, i[b][0].view(1, 1, 2)), dim=0)\n",
    "        ensemble_coor_inpaint = torch.cat((ensemble_coor_inpaint, coor_inpaint.view(1, 1, 2)), dim=0)\n",
    "        sample_count += 1\n",
    "\n",
    "        if sample_count == num_sample:\n",
    "            # Last input sequence\n",
    "            coor_zero_pad = torch.zeros((buffer_size, seq_len, 2), dtype=torch.float32)\n",
    "            coor_inpaint_buffer = torch.cat((coor_inpaint_buffer, coor_zero_pad), dim=0)\n",
    "            \n",
    "            for f in range(1, seq_len):\n",
    "                coor_inpaint = coor_inpaint_buffer[batch_i+b+f, frame_i].sum(0)\n",
    "                coor_inpaint /= (seq_len-f)\n",
    "                ensemble_i = torch.cat((ensemble_i, i[-1][f].view(1, 1, 2)), dim=0)\n",
    "                ensemble_coor_inpaint = torch.cat((ensemble_coor_inpaint, coor_inpaint.view(1, 1, 2)), dim=0)\n",
    "\n",
    "    # Thresholding\n",
    "    th_mask = ((ensemble_coor_inpaint[:, :, 0] < COOR_TH) & (ensemble_coor_inpaint[:, :, 1] < COOR_TH))\n",
    "    ensemble_coor_inpaint[th_mask] = 0.\n",
    "\n",
    "    # Predict\n",
    "    tmp_pred = predict(ensemble_i, c_pred=ensemble_coor_inpaint, img_scaler=img_scaler)\n",
    "    for key in tmp_pred.keys():\n",
    "        inpaint_pred_dict[key].extend(tmp_pred[key])\n",
    "    \n",
    "    # Update buffer, keep last predictions for ensemble in next iteration\n",
    "    coor_inpaint_buffer = coor_inpaint_buffer[-buffer_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.general import write_pred_csv\n",
    "# Write csv file\n",
    "pred_dict = inpaint_pred_dict if inpaintnet is not None else tracknet_pred_dict\n",
    "write_pred_csv(pred_dict, save_file=out_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Point:\n",
    "    x: float\n",
    "    y: float\n",
    "\n",
    "    @property\n",
    "    def int_xy_tuple(self) -> Tuple[int, int]:\n",
    "        return int(self.x), int(self.y)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Rect:\n",
    "    x: float\n",
    "    y: float\n",
    "    width: float\n",
    "    height: float\n",
    "\n",
    "    @property\n",
    "    def top_left(self) -> Point:\n",
    "        return Point(x=self.x, y=self.y)\n",
    "\n",
    "    @property\n",
    "    def bottom_right(self) -> Point:\n",
    "        return Point(x=self.x + self.width, y=self.y + self.height)\n",
    "\n",
    "    @property\n",
    "    def bottom_center(self) -> Point:\n",
    "        return Point(x=self.x + self.width / 2, y=self.y + self.height)\n",
    "    \n",
    "    \n",
    "@dataclass(frozen=True)\n",
    "class Color:\n",
    "    r: int\n",
    "    g: int\n",
    "    b: int\n",
    "\n",
    "    @property\n",
    "    def bgr_tuple(self) -> Tuple[int, int, int]:\n",
    "        return self.b, self.g, self.r\n",
    "\n",
    "def draw_rect(image: np.ndarray, rect: Rect, color: Color, thickness: int = 2) -> np.ndarray:\n",
    "    cv2.rectangle(image, rect.top_left.int_xy_tuple, rect.bottom_right.int_xy_tuple, color.bgr_tuple, thickness)\n",
    "    return image\n",
    "\n",
    "def draw_rects(frame, rects, color=Color(r=0,g=255,b=0)):\n",
    "    annotated_frame = frame\n",
    "    for rect in rects:\n",
    "        annotated_frame = draw_rect(image=annotated_frame,\n",
    "                                    rect=rect,\n",
    "                                    color=color,\n",
    "                                    thickness=2)\n",
    "    return annotated_frame\n",
    "\n",
    "def group_and_average(arr, threshold):\n",
    "    groups = []  # Store grouped numbers\n",
    "\n",
    "    current_group = [arr[0]]\n",
    "    for i in range(1, len(arr)):\n",
    "        if arr[i] - arr[i - 1] <= threshold:\n",
    "            current_group.append(arr[i])\n",
    "        else:\n",
    "            groups.append(current_group)  # Save the completed group\n",
    "            current_group = [arr[i]]  # Start a new group\n",
    "\n",
    "    groups.append(current_group)  # Add the last group\n",
    "\n",
    "    # Calculate the average for each group\n",
    "    averages = [int(np.mean(group)) for group in groups]\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get shots from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: BoundingBoxAnnotator is deprecated: `BoundingBoxAnnotator` is deprecated and has been renamed to `BoxAnnotator`. `BoundingBoxAnnotator` will be removed in supervision-0.26.0.\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}\n",
    "import supervision as sv\n",
    "import csv\n",
    "\n",
    "frame_to_xy = {}\n",
    "bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "\n",
    "with open('./TrackNetV3/prediction/rally_ball.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        frame_to_xy[int(row[\"Frame\"])] = [int(row[\"X\"]), int(row[\"Y\"])]\n",
    "\n",
    "\n",
    "frame_indices = np.arange(len(frame_to_xy))\n",
    "y = [f[1] for f in frame_to_xy.values()]\n",
    "y_1 = np.gradient(y)\n",
    "y_2 = np.gradient(y_1)\n",
    "mid_shot_frames = group_and_average([idx for idx in frame_indices if y_2[idx] < -3],threshold=5)\n",
    "shot_frames = []\n",
    "for frame in mid_shot_frames:\n",
    "    for idx in range(frame-10,frame+3):\n",
    "        shot_frames.append(idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting video into shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n",
      "done shot 8\r"
     ]
    }
   ],
   "source": [
    "%cd {HOME}\n",
    "import supervision as sv\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(video_path=\"rally.mp4\")\n",
    "\n",
    "for idx,shot_idx in enumerate(mid_shot_frames):\n",
    "    start_frame = max(shot_idx-10,0)\n",
    "    end_frame = min(shot_idx+10,video_info.total_frames - 1)\n",
    "    total_frames = end_frame - start_frame + 1\n",
    "    shot_video_info = sv.VideoInfo(width=video_info.width,height=video_info.height,fps=30,total_frames=total_frames)\n",
    "    with sv.VideoSink(target_path=f'output/shot{idx}.mp4',video_info=shot_video_info) as sink:\n",
    "        for frame in sv.get_video_frames_generator(source_path=\"rally.mp4\",start=start_frame,end=end_frame):\n",
    "            sink.write_frame(frame)\n",
    "    print(f'done shot {idx}',end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n-pose.pt\")  # load an official model\n",
    "# results = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 34.3ms\n",
      "Speed: 3.4ms preprocess, 34.3ms inference, 136.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.3ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.3ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.5ms\n",
      "Speed: 1.2ms preprocess, 6.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.3ms\n",
      "Speed: 1.2ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.3ms\n",
      "Speed: 1.2ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.3ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.3ms\n",
      "Speed: 1.2ms preprocess, 6.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.8ms\n",
      "Speed: 1.3ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.4ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.8ms\n",
      "Speed: 1.3ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.2ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.8ms\n",
      "Speed: 1.3ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.8ms\n",
      "Speed: 1.3ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.8ms\n",
      "Speed: 1.3ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.8ms\n",
      "Speed: 1.2ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.8ms\n",
      "Speed: 1.3ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.6ms\n",
      "Speed: 1.3ms preprocess, 6.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.2ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.8ms\n",
      "Speed: 1.3ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.8ms\n",
      "Speed: 1.3ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.8ms\n",
      "Speed: 1.2ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.2ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.2ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.2ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.7ms\n",
      "Speed: 1.2ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.7ms\n",
      "Speed: 1.2ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.8ms\n",
      "Speed: 1.3ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.7ms\n",
      "Speed: 1.2ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.6ms\n",
      "Speed: 1.3ms preprocess, 6.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.2ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.3ms\n",
      "Speed: 1.2ms preprocess, 6.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.3ms preprocess, 6.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.3ms\n",
      "Speed: 1.2ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.4ms\n",
      "Speed: 1.2ms preprocess, 6.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.5ms\n",
      "Speed: 1.3ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7.5ms\n",
      "Speed: 1.2ms preprocess, 7.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.4ms\n",
      "Speed: 1.9ms preprocess, 6.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.5ms\n",
      "Speed: 1.3ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.5ms\n",
      "Speed: 1.3ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.5ms\n",
      "Speed: 1.3ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.7ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.7ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 2.0ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.8ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.3ms\n",
      "Speed: 1.9ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.3ms\n",
      "Speed: 1.2ms preprocess, 6.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import supervision as sv\n",
    "def callback(frame, idx):\n",
    "    x,y = frame_to_xy[idx]\n",
    "    color = Color(r=255,g=0,b=0) if idx in shot_frames else Color(r=0,g=255,b=0)\n",
    "    annotated_frame = draw_rects(frame, [Rect(x-10,y-10,20,20)],color=color)\n",
    "    results = model(frame)\n",
    "    return annotated_frame\n",
    "\n",
    "\n",
    "sv.process_video(source_path='./rally.mp4', target_path='./output/rally_annotated.mp4', callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Shuttle Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "frame_to_xy = {}\n",
    "\n",
    "with open('./TrackNetV3/prediction/rally_ball.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        frame_to_xy[int(row[\"Frame\"])] = [int(row[\"X\"]), int(row[\"Y\"])]\n",
    "\n",
    "frame = np.arange(len(frame_to_xy))\n",
    "x = [f[0] for f in frame_to_xy.values()]\n",
    "y = [f[1] for f in frame_to_xy.values()]\n",
    "y_1 = np.gradient(y)\n",
    "y_2 = np.gradient(y_1)\n",
    "shot_frames = [idx for idx in frame if y_2[idx] < -3]\n",
    "# print()\n",
    "# # print(np.where(np.diff(np.sign(y_2)) != 0))\n",
    "# y_3 = np.gradient(y_2)\n",
    "\n",
    "\n",
    "# # plt.plot(frame, x,color=\"red\")\n",
    "# # plt.plot(frame, y,color=\"green\")\n",
    "# # plt.plot(frame, y_1,color=\"blue\")\n",
    "# plt.plot(frame, y_2,color=\"purple\")\n",
    "# # plt.plot(frame, y_3,color=\"red\")\n",
    "# # plt.show()\n",
    "\n",
    "# 30 f / s, frame \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
